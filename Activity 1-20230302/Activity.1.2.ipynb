{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1.2: Bootstrap\n",
    "Last modified (13 Feb 2023)\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "In this activity we learn how to:\n",
    "\n",
    "- define a repeated data splitting (cross validation) scheme in `scikit-learn`\n",
    "- implement the Bootstrap in the splitter framework (using `numpy`)\n",
    "- apply the Bootstrap to more reliably estimate the effect of a hyper-parameter (using [`sklearn.model_selection.cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate))\n",
    "- plot the results of an experiment with error bars using `matplotlib`\n",
    "- choose a suitable number of Bootstrap repetitions\n",
    "- describe how the number of training data influences the choice for $k$ in $k$-NN\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Activity 1.1\n",
    "- Lecture 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous activity we have determined that drawing conclusions about predictive performance from a single train/test split is risky, because our error estimates will often have a large variance. Here, the source of randomness is the specific split but also the overall data sample we got in the first place (imagine Fisher would have obtained a second sample of Iris flowers; would it have been identical to the first one?). \n",
    "\n",
    "Therefore, we now look at more systematic to obtain error estimates that take this variation over possible training sets into account. In Chapter 3, we have learned that Bootstrapping is a powerful statistical tool that helps us to measure the uncertainty in the prediction of a model. In this activity, we implement this technique to assess variations in the prediction of KNN classifier. In **Assignment Task 2.B.**, you will be asked to expand this example and develop a bootstrap procedure for KNN as a regressor. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "- Before turning to the Bootstrap, we will first learn how data splitting and subsampling strategies are generally described and implemented in `scikit-learn`. \n",
    "- We will demonstrate that using again the Iris dataset and the $k$-NN classifier. Instead of re-implementing $k$-NN, this time we will simply use the implementation from `scikit-learn`.\n",
    "- Then we will implement a Bootstrap splitting scheme and evaluate the effect of the number of repetitions and training size in determining what is the best value for $k$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitters in `scikit-learn`\n",
    "\n",
    "In the framework of `scikit-learn`, the Boostrap can be considered a particular cross validation scheme where we repeatedly generate training sets via index sampling with replacement and use the unsampled indices in a given iteration as test indices. \n",
    "\n",
    "Such schemes are implemented in `skikit-learn` as `Splitter` objects that provide two methods:\n",
    "\n",
    "- `get_n_splits(x=None, y=None, groups=None)` which informs about how many splits the splitter would produce for the given dataset; here all parameters are optional as the splits (often) do not depend on the given number\n",
    "- `split(x, y=None, groups=None)` which is a [generator function](https://docs.python.org/3/reference/expressions.html?highlight=yield%20statement#yield-expressions) that yields a sequence of training and test indices\n",
    "\n",
    "As an illustration let us look at the class `ShuffleSplit` and use it with the Iris dataset. This splitting scheme allows as to perform a simple training/test split akin to what we have implemented in Activity 1.1. However, it conveniently allows for repetitions of the splitting process to increase our confidence in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "iris = load_iris()\n",
    "shuffleSplit = ShuffleSplit(n_splits=2, train_size=0.6, random_state=0)\n",
    "print('Shuffle split will generate', shuffleSplit.get_n_splits(), 'splits.')\n",
    "for i, (train_idx, test_idx) in enumerate(shuffleSplit.split(iris.data)):\n",
    "    print('split', i)\n",
    "    print('train indices', train_idx)\n",
    "    print('test indices', test_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could use such a splitter in an explicit loop as above to run an experiment, it is often more convenient to use the function [`sklearn.model_selection.cross_valide`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate). Let us demonstrate this function with 5 repetitions of shuffle splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(3)\n",
    "shuffleSplit5 = ShuffleSplit(n_splits=5, train_size=0.6, random_state=0)\n",
    "cross_validate(knn, iris.data, iris.target, cv=shuffleSplit5, return_train_score=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `cross_validate` returns a dictionary that not only contains the test and train prediction metrics for all repetitions defined by the splitter but also the time that was needed for fitting and prediction (which will be useful when aiming to optimise an implementation.\n",
    "\n",
    "One thing that is noteworthy is that the metric used here is not the error rate but the the inverse accuracy metric, i.e., one minus the error rate. Below we show how we can provide our own metric based on providing a per-data-point error (or loss) function. Here, we use the zero-one error, which results in our familiar error rate as score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, zero_one_loss\n",
    "\n",
    "cross_validate(knn, iris.data, iris.target, cv=shuffleSplit5, return_train_score=True, scoring=make_scorer(zero_one_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Bootstrap\n",
    "\n",
    "#### Task A: Complete the Implementation of the bootstrap splitter\n",
    "\n",
    "**Insert the line in the Bootstrap splitter implementation below that generates the train indices in a given iteration**\n",
    "\n",
    "Hint: You can go back to Activity 1.1 to remind yourself how we have performed the single train/test split. What parameter has to change to sample the training indices for the Bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BootstrapSplitter:\n",
    "\n",
    "    def __init__(self, reps, train_size, random_state=None):\n",
    "        self.reps = reps\n",
    "        self.train_size = train_size\n",
    "        self.RNG = np.random.default_rng(random_state)\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.reps\n",
    "\n",
    "    def split(self, x, y=None, groups=None):\n",
    "        for _ in range(self.reps):\n",
    "            train_idx = # YOUR CODE HERE\n",
    "            test_idx = np.setdiff1d(np.arange(len(x)), train_idx)\n",
    "            np.random.shuffle(test_idx)\n",
    "            yield train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap = BootstrapSplitter(5, 0.6, random_state=0)\n",
    "cross_validate(knn, iris.data, iris.target, cv=bootstrap, return_train_score=True, scoring=make_scorer(zero_one_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Test the Effect of $k$ in $k$-NN\n",
    "\n",
    "The following function allows us to run a series of bootstrap experiments, one for each value of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(max_k, cv):\n",
    "    r = cv.get_n_splits()\n",
    "    test_results = np.zeros(shape=(r, max_k))\n",
    "    train_results = np.zeros(shape=(r, max_k))\n",
    "    for k in range(1, max_k+1):\n",
    "        knn = KNeighborsClassifier(k)\n",
    "        cv_res = cross_validate(knn, iris.data, iris.target, cv=cv, return_train_score=True, scoring=make_scorer(zero_one_loss))\n",
    "        test_results[:, k-1] = cv_res['test_score']\n",
    "        train_results[:, k-1] = cv_res['train_score']\n",
    "\n",
    "    return train_results, test_results\n",
    "\n",
    "reps=25\n",
    "max_k=30\n",
    "train_results, test_results = evaluation(max_k=max_k, cv=BootstrapSplitter(reps, 0.6, random_state=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results\n",
    "\n",
    "When visualising the results of the experiments we now can compute meaningful error bars for each estimated performance due to the performed repetitions. This can be done with the `matplotlib`-function `errorbar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ks = np.arange(1, max_k+1)\n",
    "_, axs = plt.subplots(1, 2, figsize=(8,4), tight_layout=True, sharey=True)\n",
    "z = (reps**0.5)/1.96\n",
    "axs[0].errorbar(ks, train_results.mean(axis=0), yerr=train_results.std(axis=0)/z, marker='o', label='train')\n",
    "axs[0].errorbar(ks, test_results.mean(axis=0), yerr=test_results.std(axis=0)/z, marker='o', label='test')\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('$k$')\n",
    "axs[0].set_ylabel('error rate')\n",
    "axs[1].errorbar(1/ks, train_results.mean(axis=0), yerr=train_results.std(axis=0)/z, marker='o', label='train')\n",
    "axs[1].errorbar(1/ks, test_results.mean(axis=0), yerr=test_results.std(axis=0)/z, marker='o', label='test')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('$1/k$')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative visualisation that uses [boxplots](https://en.wikipedia.org/wiki/Box_plot), which can be plotted with the function [`matplotlib.pyplot.boxplot`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html). Since there are more details about the result distribution in a boxplot it makes sense to focus on one individual metric in such a plot (here we use the test error rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(test_results, showfliers=False)\n",
    "plt.ylabel('test error rate')\n",
    "plt.xlabel('$k$')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Bootstrap Repetitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task B: Give a hypothesis and rationale how changing the number of bootstrap repetitions will change the results of the experiment\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n",
    "\n",
    "#### Task C: Test your hypothesis by completing the experiment code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = [25, 50, 100]\n",
    "train_results_reps = []\n",
    "test_results_reps = []\n",
    "\n",
    "for r in repetitions:\n",
    "    print('reps: ', r)\n",
    "    train_res, test_res = # YOUR CODE HERE\n",
    "    train_results_reps.append(train_res)\n",
    "    test_results_reps.append(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1, max_k+1)\n",
    "_, axs = plt.subplots(1, len(repetitions), figsize=(4*len(repetitions),4), tight_layout=True, sharey=True)\n",
    "for i, r in enumerate(repetitions):\n",
    "    z = r**0.5/1.96\n",
    "    axs[i].set_title(f'$r$={r}')\n",
    "    axs[i].errorbar(ks, train_results_reps[i].mean(axis=0), yerr=train_results_reps[i].std(axis=0)/z, marker='o', label='train')\n",
    "    axs[i].errorbar(ks, test_results_reps[i].mean(axis=0), yerr=test_results_reps[i].std(axis=0)/z, marker='o', label='test')\n",
    "    axs[i].set_xlabel('$k$')\n",
    "axs[0].legend()    \n",
    "axs[0].set_ylabel('error rate')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Train Size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task D: Give a hypothesis and rationale how changing the train size will change the results of the experiment and in particular how it will influence what $k$ will achieve the best test performance.\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [0.4, 0.6, 0.8]\n",
    "train_results_trainsize = []\n",
    "test_results_trainsize = []\n",
    "reps = 100\n",
    "\n",
    "for frac in training_sizes:\n",
    "    print('train size: ', round(len(iris.data)*frac))\n",
    "    train_res, test_res = evaluation(max_k, BootstrapSplitter(reps, frac, random_state=1))\n",
    "    train_results_trainsize.append(train_res)\n",
    "    test_results_trainsize.append(test_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Determine the best $k$ per train size.\n",
    "\n",
    "**Add code below to compile the list of optimal $k$ values corresponding to the train sizes.**\n",
    "\n",
    "Hint: Check the documentation of [`np.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ks = # YOUR CODE HERE\n",
    "best_ks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot all results including your determined optimal $k$ values to check your hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1, max_k+1)\n",
    "_, axs = plt.subplots(1, len(training_sizes), figsize=(4*len(training_sizes),4), tight_layout=True, sharey=True)\n",
    "for i, frac in enumerate(training_sizes):\n",
    "    z = reps**0.5/1.96\n",
    "    axs[i].set_title(f'$n$={round(len(iris.data)*frac)}')\n",
    "    axs[i].errorbar(ks, train_results_trainsize[i].mean(axis=0), yerr=train_results_trainsize[i].std(axis=0)/z, marker='o', label='train')\n",
    "    axs[i].errorbar(ks, test_results_trainsize[i].mean(axis=0), yerr=test_results_trainsize[i].std(axis=0)/z, marker='o', label='test')\n",
    "    axs[i].axvline(ks[best_ks[i]], color='black', linestyle='--', label='best $k$')\n",
    "    axs[i].set_xlabel('$k$')\n",
    "axs[0].legend()    \n",
    "axs[0].set_ylabel('error rate')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "#### Task F: Summarise what you have learned by briefly answering the following questions.\n",
    "\n",
    "**What is the purpose of the Bootstrap when evaluating machine learning algorithms and how should you choose the number of Bootstrap repetitions?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*\n",
    "\n",
    "**How does the number of available training data points influence the choice of the hyper-parameter $k$ in $k$-NN, and what is the reason for this?**\n",
    "\n",
    "*[YOUR ANSWER HERE]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
